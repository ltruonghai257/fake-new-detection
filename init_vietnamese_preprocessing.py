#!/usr/bin/env python3
"""
Initialize Vietnamese Text Preprocessing
Usage example for Vietnamese fake news detection
"""

import sys
import os
import json
import numpy as np

# Add src to path
sys.path.append('./src')

from preprocessing.text_preprocessing import TextPreprocessor, preprocess_vietnamese_data

def init_preprocessing():
    """Initialize Vietnamese text preprocessing with sample data"""
    
    print("ğŸ‡»ğŸ‡³ Initializing Vietnamese Text Preprocessing for COOLANT")
    print("=" * 60)
    
    # Initialize preprocessor for Vietnamese
    preprocessor = TextPreprocessor(
        model_name="vinai/phobert-base",  # Vietnamese BERT model
        language="vi",
        max_length=512,
        device="cuda" if os.system("nvidia-smi") == 0 else "cpu"
    )
    
    print(f"âœ“ Preprocessor initialized with model: {preprocessor.model_name}")
    print(f"âœ“ Device: {preprocessor.device}")
    print(f"âœ“ Max sequence length: {preprocessor.max_length}")
    
    # Sample Vietnamese texts for testing
    sample_texts = [
        "Tin tá»©c Viá»‡t Nam hÃ´m nay: ChÃ­nh phá»§ ban hÃ nh chÃ­nh sÃ¡ch má»›i vá» kinh táº¿.",
        "Cáº£nh bÃ¡o: Tin giáº£ vá» dá»‹ch bá»‡nh COVID-19 Ä‘ang lan truyá»n trÃªn máº¡ng xÃ£ há»™i.",
        "Khoa há»c cÃ´ng nghá»‡ Viá»‡t Nam Ä‘áº¡t nhiá»u thÃ nh tá»±u quan trá»ng trong nÄƒm 2023.",
        "BREAKING: PhÃ¡t hiá»‡n thuá»‘c chá»¯a bÃ¡ch bá»‡nh - cÃ¡c chuyÃªn gia cáº£nh bÃ¡o tin giáº£."
    ]
    
    sample_labels = [0, 1, 0, 1]  # 0: real news, 1: fake news
    
    print(f"\nğŸ“ Processing {len(sample_texts)} sample Vietnamese texts...")
    
    # Test text cleaning
    print("\nOriginal texts:")
    for i, text in enumerate(sample_texts):
        print(f"  {i+1}. {text}")
    
    print("\nCleaned texts:")
    for i, text in enumerate(sample_texts):
        cleaned = preprocessor.clean_text(text)
        print(f"  {i+1}. {cleaned}")
    
    # Extract features
    print("\nğŸ”§ Extracting BERT features...")
    
    # Option 1: BERT features (pooled output)
    bert_features = preprocessor.extract_bert_features(sample_texts)
    print(f"âœ“ BERT features shape: {bert_features.shape}")
    
    # Option 2: Token embeddings (for FastCNN)
    token_embeddings = preprocessor.extract_token_embeddings(sample_texts)
    print(f"âœ“ Token embeddings shape: {token_embeddings.shape}")
    
    # Save processed data
    print("\nğŸ’¾ Saving processed data...")
    
    # Create output directory
    os.makedirs("./processed_data", exist_ok=True)
    
    # Save BERT features
    preprocessor.save_preprocessed_data(
        bert_features, 
        np.array(sample_labels), 
        "./processed_data/vietnamese_bert_features.pkl"
    )
    
    # Save token embeddings
    preprocessor.save_preprocessed_data(
        token_embeddings, 
        np.array(sample_labels), 
        "./processed_data/vietnamese_token_embeddings.pkl"
    )
    
    print("\nğŸ‰ Vietnamese text preprocessing initialization completed!")
    print("\nğŸ“ Files created:")
    print("  - ./processed_data/vietnamese_bert_features.pkl")
    print("  - ./processed_data/vietnamese_token_embeddings.pkl")
    
    return preprocessor

def process_custom_dataset(data_path: str):
    """Process custom Vietnamese dataset from JSON file"""
    
    print(f"\nğŸ“‚ Processing dataset from: {data_path}")
    
    try:
        # Load Vietnamese dataset
        with open(data_path, 'r', encoding='utf-8') as f:
            raw_data = json.load(f)
        
        # Extract texts and labels (adjust based on your data structure)
        if isinstance(raw_data, list):
            texts = [item.get('text', item.get('content', '')) for item in raw_data]
            labels = [item.get('label', item.get('is_fake', 0)) for item in raw_data]
        elif isinstance(raw_data, dict):
            texts = raw_data.get('texts', [])
            labels = raw_data.get('labels', [])
        else:
            raise ValueError("Unsupported data format")
        
        print(f"âœ“ Loaded {len(texts)} texts and {len(labels)} labels")
        
        # Initialize preprocessor
        preprocessor = TextPreprocessor(
            model_name="vinai/phobert-base",
            language="vi"
        )
        
        # Process dataset
        features, processed_labels = preprocessor.preprocess_dataset(
            texts, labels,
            save_path="./processed_data/custom_vietnamese_dataset.pkl",
            extract_type="token_embeddings"  # FastCNN compatible
        )
        
        print(f"âœ“ Processed features shape: {features.shape}")
        print(f"âœ“ Saved to: ./processed_data/custom_vietnamese_dataset.pkl")
        
        return features, processed_labels
        
    except Exception as e:
        print(f"âŒ Error processing dataset: {e}")
        return None, None

def main():
    """Main function to run initialization"""
    
    # Create output directory
    os.makedirs("./processed_data", exist_ok=True)
    
    # Run initialization
    preprocessor = init_preprocessing()
    
    # Optional: Process custom dataset
    custom_data_path = "./src/data/json/news_data_vifactcheck_dev.json"
    if os.path.exists(custom_data_path):
        print("\n" + "="*60)
        print("ğŸ¯ Processing your Vietnamese dataset...")
        features, labels = process_custom_dataset(custom_data_path)
        
        if features is not None:
            print(f"\nğŸš€ Your Vietnamese dataset is ready for COOLANT training!")
            print(f"   Features shape: {features.shape}")
            print(f"   Labels shape: {labels.shape}")
    else:
        print(f"\nâš ï¸  Custom dataset not found at: {custom_data_path}")
        print("   You can manually process your dataset using the process_custom_dataset() function")
    
    print("\nğŸ‰ Vietnamese preprocessing setup completed!")

if __name__ == "__main__":
    main()
